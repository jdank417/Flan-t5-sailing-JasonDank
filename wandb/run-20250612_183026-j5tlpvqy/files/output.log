2025-06-12 18:30:26,914 - SailingRulesTrainer - INFO - Weights & Biases initialized. Project: sailing-rules-assistant, Run: flan-t5-16-epochs-10
Weights & Biases initialized. Project: sailing-rules-assistant, Run: flan-t5-16-epochs-10
wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
Track your training at: https://wandb.ai/dankj-jason-dank/sailing-rules-assistant/runs/j5tlpvqy
2025-06-12 18:30:26,915 - SailingRulesTrainer - INFO - Starting training preparation
Map:   0%|          | 0/323 [00:00<?, ? examples/s]/Users/jasondank/PycharmProjects/Flan-t5-sailing/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|██████████| 323/323 [00:00<00:00, 8901.53 examples/s]
2025-06-12 18:30:30,294 - SailingRulesTrainer - INFO - Using device: cpu
2025-06-12 18:30:30,311 - SailingRulesTrainer - WARNING - Warning: fp16 mixed precision is not supported on this device. Training with fp32 instead.
2025-06-12 18:30:30,311 - SailingRulesTrainer - INFO - Validation dataset is empty. Training without validation.
/Users/jasondank/PycharmProjects/Flan-t5-sailing/models/trainer.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(**trainer_kwargs)
You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-06-12 18:30:30,809 - SailingRulesTrainer - INFO - Starting training with 323 examples for 10 epochs

==================================================
TRAINING STARTED
==================================================
Training logs are being saved to: training.log
Track training progress in real-time at: https://wandb.ai/dankj-jason-dank/sailing-rules-assistant/runs/j5tlpvqy
You can safely close this terminal - training will continue in the background.
==================================================
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|          | 0/810 [00:00<?, ?it/s]/Users/jasondank/PycharmProjects/Flan-t5-sailing/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
 16%|█▋        | 132/810 [03:12<16:31,  1.46s/it]

{'loss': 35.6486, 'grad_norm': 6.107494831085205, 'learning_rate': 4.5e-06, 'epoch': 0.12}
{'loss': 35.5421, 'grad_norm': 6.388044834136963, 'learning_rate': 9.5e-06, 'epoch': 0.25}
{'loss': 36.2097, 'grad_norm': 6.441753387451172, 'learning_rate': 1.45e-05, 'epoch': 0.37}
{'loss': 35.1986, 'grad_norm': 5.028326511383057, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.49}
{'loss': 35.464, 'grad_norm': 8.713080406188965, 'learning_rate': 2.45e-05, 'epoch': 0.62}
{'loss': 34.6338, 'grad_norm': 7.477224349975586, 'learning_rate': 2.95e-05, 'epoch': 0.74}
{'loss': 34.4734, 'grad_norm': 7.7628703117370605, 'learning_rate': 3.45e-05, 'epoch': 0.86}
{'loss': 33.7245, 'grad_norm': 9.54727554321289, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.99}
{'loss': 32.4982, 'grad_norm': 8.38467788696289, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.11}
{'loss': 31.3469, 'grad_norm': 10.848560333251953, 'learning_rate': 4.9500000000000004e-05, 'epoch': 1.23}
{'loss': 30.1997, 'grad_norm': 10.460179328918457, 'learning_rate': 4.936619718309859e-05, 'epoch': 1.36}
{'loss': 29.3491, 'grad_norm': 7.525224685668945, 'learning_rate': 4.866197183098592e-05, 'epoch': 1.48}
{'loss': 27.6407, 'grad_norm': 7.439520835876465, 'learning_rate': 4.7957746478873244e-05, 'epoch': 1.6}
