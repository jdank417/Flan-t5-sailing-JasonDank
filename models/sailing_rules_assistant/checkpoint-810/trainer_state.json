{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 810,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 6.38567590713501,
      "learning_rate": 4.5e-06,
      "loss": 35.6484,
      "step": 10
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 6.580613613128662,
      "learning_rate": 9.5e-06,
      "loss": 35.5413,
      "step": 20
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 6.783841609954834,
      "learning_rate": 1.45e-05,
      "loss": 36.2059,
      "step": 30
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 5.202436447143555,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 35.194,
      "step": 40
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 8.907642364501953,
      "learning_rate": 2.45e-05,
      "loss": 35.4505,
      "step": 50
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 7.433456897735596,
      "learning_rate": 2.95e-05,
      "loss": 34.6165,
      "step": 60
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 7.936572551727295,
      "learning_rate": 3.45e-05,
      "loss": 34.4507,
      "step": 70
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 9.694764137268066,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 33.6843,
      "step": 80
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 8.386953353881836,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 32.4518,
      "step": 90
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 10.75993537902832,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 31.2875,
      "step": 100
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 10.363693237304688,
      "learning_rate": 4.936619718309859e-05,
      "loss": 30.1405,
      "step": 110
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 7.476474761962891,
      "learning_rate": 4.866197183098592e-05,
      "loss": 29.2738,
      "step": 120
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 7.350247859954834,
      "learning_rate": 4.7957746478873244e-05,
      "loss": 27.5486,
      "step": 130
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 8.423116683959961,
      "learning_rate": 4.725352112676056e-05,
      "loss": 25.9288,
      "step": 140
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 8.83059024810791,
      "learning_rate": 4.654929577464789e-05,
      "loss": 24.5505,
      "step": 150
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 7.328764915466309,
      "learning_rate": 4.5845070422535214e-05,
      "loss": 23.2387,
      "step": 160
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 10.48979377746582,
      "learning_rate": 4.514084507042254e-05,
      "loss": 21.1305,
      "step": 170
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 8.790948867797852,
      "learning_rate": 4.4436619718309865e-05,
      "loss": 19.5209,
      "step": 180
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 9.378870964050293,
      "learning_rate": 4.373239436619718e-05,
      "loss": 17.6743,
      "step": 190
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 10.439240455627441,
      "learning_rate": 4.302816901408451e-05,
      "loss": 15.3897,
      "step": 200
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 10.854515075683594,
      "learning_rate": 4.2323943661971834e-05,
      "loss": 12.8382,
      "step": 210
    },
    {
      "epoch": 2.7160493827160495,
      "grad_norm": 10.412023544311523,
      "learning_rate": 4.161971830985916e-05,
      "loss": 10.2751,
      "step": 220
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 5.065361499786377,
      "learning_rate": 4.091549295774648e-05,
      "loss": 8.2972,
      "step": 230
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 3.8519716262817383,
      "learning_rate": 4.0211267605633804e-05,
      "loss": 6.7661,
      "step": 240
    },
    {
      "epoch": 3.0864197530864197,
      "grad_norm": 2.9667561054229736,
      "learning_rate": 3.950704225352112e-05,
      "loss": 5.836,
      "step": 250
    },
    {
      "epoch": 3.2098765432098766,
      "grad_norm": 1.585923194885254,
      "learning_rate": 3.880281690140845e-05,
      "loss": 5.4056,
      "step": 260
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 1.181266188621521,
      "learning_rate": 3.8098591549295774e-05,
      "loss": 5.1797,
      "step": 270
    },
    {
      "epoch": 3.45679012345679,
      "grad_norm": 0.8372135162353516,
      "learning_rate": 3.73943661971831e-05,
      "loss": 5.0752,
      "step": 280
    },
    {
      "epoch": 3.580246913580247,
      "grad_norm": 0.8026716113090515,
      "learning_rate": 3.6690140845070425e-05,
      "loss": 4.9594,
      "step": 290
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 0.851057231426239,
      "learning_rate": 3.598591549295775e-05,
      "loss": 4.8514,
      "step": 300
    },
    {
      "epoch": 3.8271604938271606,
      "grad_norm": 0.6432007551193237,
      "learning_rate": 3.528169014084507e-05,
      "loss": 4.8361,
      "step": 310
    },
    {
      "epoch": 3.950617283950617,
      "grad_norm": 0.6543511152267456,
      "learning_rate": 3.4577464788732395e-05,
      "loss": 4.7438,
      "step": 320
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 0.68235182762146,
      "learning_rate": 3.387323943661972e-05,
      "loss": 4.72,
      "step": 330
    },
    {
      "epoch": 4.197530864197531,
      "grad_norm": 0.5799514651298523,
      "learning_rate": 3.3169014084507046e-05,
      "loss": 4.643,
      "step": 340
    },
    {
      "epoch": 4.320987654320987,
      "grad_norm": 0.6096490621566772,
      "learning_rate": 3.246478873239437e-05,
      "loss": 4.5879,
      "step": 350
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.8586055636405945,
      "learning_rate": 3.17605633802817e-05,
      "loss": 4.5495,
      "step": 360
    },
    {
      "epoch": 4.567901234567901,
      "grad_norm": 0.6660680174827576,
      "learning_rate": 3.1056338028169016e-05,
      "loss": 4.4945,
      "step": 370
    },
    {
      "epoch": 4.6913580246913575,
      "grad_norm": 0.9811722040176392,
      "learning_rate": 3.0352112676056338e-05,
      "loss": 4.4596,
      "step": 380
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 0.8145490288734436,
      "learning_rate": 2.9647887323943664e-05,
      "loss": 4.4068,
      "step": 390
    },
    {
      "epoch": 4.938271604938271,
      "grad_norm": 0.9935279488563538,
      "learning_rate": 2.894366197183099e-05,
      "loss": 4.3228,
      "step": 400
    },
    {
      "epoch": 5.061728395061729,
      "grad_norm": 1.3434652090072632,
      "learning_rate": 2.823943661971831e-05,
      "loss": 4.2938,
      "step": 410
    },
    {
      "epoch": 5.185185185185185,
      "grad_norm": 1.1235113143920898,
      "learning_rate": 2.7535211267605637e-05,
      "loss": 4.2426,
      "step": 420
    },
    {
      "epoch": 5.308641975308642,
      "grad_norm": 1.1462819576263428,
      "learning_rate": 2.6830985915492955e-05,
      "loss": 4.075,
      "step": 430
    },
    {
      "epoch": 5.432098765432099,
      "grad_norm": 1.5153378248214722,
      "learning_rate": 2.612676056338028e-05,
      "loss": 4.0074,
      "step": 440
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 2.7048864364624023,
      "learning_rate": 2.5422535211267607e-05,
      "loss": 4.0139,
      "step": 450
    },
    {
      "epoch": 5.679012345679013,
      "grad_norm": 1.7077869176864624,
      "learning_rate": 2.4718309859154932e-05,
      "loss": 3.8687,
      "step": 460
    },
    {
      "epoch": 5.802469135802469,
      "grad_norm": 2.790414810180664,
      "learning_rate": 2.4014084507042258e-05,
      "loss": 3.7929,
      "step": 470
    },
    {
      "epoch": 5.925925925925926,
      "grad_norm": 2.169116735458374,
      "learning_rate": 2.3309859154929576e-05,
      "loss": 3.7621,
      "step": 480
    },
    {
      "epoch": 6.049382716049383,
      "grad_norm": 2.519355535507202,
      "learning_rate": 2.2605633802816902e-05,
      "loss": 3.7195,
      "step": 490
    },
    {
      "epoch": 6.172839506172839,
      "grad_norm": 1.1268924474716187,
      "learning_rate": 2.1901408450704227e-05,
      "loss": 3.661,
      "step": 500
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 1.3238317966461182,
      "learning_rate": 2.119718309859155e-05,
      "loss": 3.551,
      "step": 510
    },
    {
      "epoch": 6.419753086419753,
      "grad_norm": 1.3197295665740967,
      "learning_rate": 2.0492957746478875e-05,
      "loss": 3.4664,
      "step": 520
    },
    {
      "epoch": 6.54320987654321,
      "grad_norm": 1.8263617753982544,
      "learning_rate": 1.9788732394366197e-05,
      "loss": 3.4437,
      "step": 530
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 2.461974620819092,
      "learning_rate": 1.9084507042253523e-05,
      "loss": 3.4565,
      "step": 540
    },
    {
      "epoch": 6.790123456790123,
      "grad_norm": 2.105285406112671,
      "learning_rate": 1.8380281690140845e-05,
      "loss": 3.4019,
      "step": 550
    },
    {
      "epoch": 6.91358024691358,
      "grad_norm": 1.3768517971038818,
      "learning_rate": 1.7676056338028167e-05,
      "loss": 3.3605,
      "step": 560
    },
    {
      "epoch": 7.037037037037037,
      "grad_norm": 2.237753391265869,
      "learning_rate": 1.6971830985915493e-05,
      "loss": 3.3426,
      "step": 570
    },
    {
      "epoch": 7.160493827160494,
      "grad_norm": 2.0499415397644043,
      "learning_rate": 1.6267605633802818e-05,
      "loss": 3.323,
      "step": 580
    },
    {
      "epoch": 7.283950617283951,
      "grad_norm": 1.7262005805969238,
      "learning_rate": 1.556338028169014e-05,
      "loss": 3.3378,
      "step": 590
    },
    {
      "epoch": 7.407407407407407,
      "grad_norm": 1.345441460609436,
      "learning_rate": 1.4859154929577466e-05,
      "loss": 3.2063,
      "step": 600
    },
    {
      "epoch": 7.530864197530864,
      "grad_norm": 1.2652385234832764,
      "learning_rate": 1.415492957746479e-05,
      "loss": 3.2082,
      "step": 610
    },
    {
      "epoch": 7.654320987654321,
      "grad_norm": 2.188324451446533,
      "learning_rate": 1.3450704225352112e-05,
      "loss": 3.0874,
      "step": 620
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 1.5065003633499146,
      "learning_rate": 1.2746478873239437e-05,
      "loss": 3.1439,
      "step": 630
    },
    {
      "epoch": 7.901234567901234,
      "grad_norm": 1.605648159980774,
      "learning_rate": 1.2042253521126761e-05,
      "loss": 3.0726,
      "step": 640
    },
    {
      "epoch": 8.024691358024691,
      "grad_norm": 1.3507509231567383,
      "learning_rate": 1.1338028169014085e-05,
      "loss": 3.0329,
      "step": 650
    },
    {
      "epoch": 8.148148148148149,
      "grad_norm": 2.1504156589508057,
      "learning_rate": 1.0633802816901409e-05,
      "loss": 3.0785,
      "step": 660
    },
    {
      "epoch": 8.271604938271604,
      "grad_norm": 2.0683579444885254,
      "learning_rate": 9.929577464788733e-06,
      "loss": 3.0182,
      "step": 670
    },
    {
      "epoch": 8.395061728395062,
      "grad_norm": 1.800345778465271,
      "learning_rate": 9.225352112676057e-06,
      "loss": 3.0312,
      "step": 680
    },
    {
      "epoch": 8.518518518518519,
      "grad_norm": 1.9392293691635132,
      "learning_rate": 8.52112676056338e-06,
      "loss": 2.9491,
      "step": 690
    },
    {
      "epoch": 8.641975308641975,
      "grad_norm": 2.1425631046295166,
      "learning_rate": 7.816901408450706e-06,
      "loss": 3.0113,
      "step": 700
    },
    {
      "epoch": 8.765432098765432,
      "grad_norm": 1.4805538654327393,
      "learning_rate": 7.112676056338029e-06,
      "loss": 2.9536,
      "step": 710
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 1.4991929531097412,
      "learning_rate": 6.408450704225352e-06,
      "loss": 2.9385,
      "step": 720
    },
    {
      "epoch": 9.012345679012345,
      "grad_norm": 1.7438308000564575,
      "learning_rate": 5.7042253521126766e-06,
      "loss": 2.9386,
      "step": 730
    },
    {
      "epoch": 9.135802469135802,
      "grad_norm": 2.381476640701294,
      "learning_rate": 5e-06,
      "loss": 2.9549,
      "step": 740
    },
    {
      "epoch": 9.25925925925926,
      "grad_norm": 1.6449942588806152,
      "learning_rate": 4.295774647887324e-06,
      "loss": 2.8991,
      "step": 750
    },
    {
      "epoch": 9.382716049382717,
      "grad_norm": 1.4767013788223267,
      "learning_rate": 3.591549295774648e-06,
      "loss": 2.9105,
      "step": 760
    },
    {
      "epoch": 9.506172839506172,
      "grad_norm": 1.6541101932525635,
      "learning_rate": 2.887323943661972e-06,
      "loss": 2.931,
      "step": 770
    },
    {
      "epoch": 9.62962962962963,
      "grad_norm": 1.9618717432022095,
      "learning_rate": 2.1830985915492958e-06,
      "loss": 2.9329,
      "step": 780
    },
    {
      "epoch": 9.753086419753087,
      "grad_norm": 1.6020835638046265,
      "learning_rate": 1.4788732394366198e-06,
      "loss": 2.8772,
      "step": 790
    },
    {
      "epoch": 9.876543209876543,
      "grad_norm": 1.5933053493499756,
      "learning_rate": 7.746478873239438e-07,
      "loss": 2.8379,
      "step": 800
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.617263913154602,
      "learning_rate": 7.042253521126761e-08,
      "loss": 2.9144,
      "step": 810
    }
  ],
  "logging_steps": 10,
  "max_steps": 810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2229323940495360.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
